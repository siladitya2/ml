{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X,Y = np.loadtxt(\"pizza.txt\",skiprows=1,unpack=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13.,  2., 14., 23., 13., 13.,  1., 18.,  7., 10., 26.,  3.,  3.,\n",
       "       21., 22.,  2., 27.,  6., 10., 18., 15.,  9., 26.,  8., 15., 10.,\n",
       "       21.,  5.,  6., 13.])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([33., 16., 32., 51., 27., 25., 16., 34., 22., 17., 29., 15., 15.,\n",
       "       32., 37., 13., 44., 16., 21., 37., 30., 26., 34., 23., 39., 27.,\n",
       "       37., 17., 18., 23.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.xlabel(\"Reservations\",fontsize=20)\n",
    "plt.ylabel(\"Pizzas\",fontsize=20)\n",
    "plt.axis([0,50,0,50])\n",
    "plt.plot(X,Y,\"bo\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, w, b):\n",
    " return X * w + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.8\n"
     ]
    }
   ],
   "source": [
    "print(predict(14, 1.2, 12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([28.8, 18. , 20.4])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([14, 5, 7])\n",
    "predict(X, 1.2, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(X, Y, w, b):\n",
    " predictions = predict(X, w, b)\n",
    " return np.average((predictions - Y) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = np.loadtxt(\"pizza.txt\", skiprows=1, unpack=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23.778666666666677"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(X, Y, 1.2, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, Y, iterations, lr):\n",
    "    w = b = 0\n",
    "    for i in range(iterations):\n",
    "        current_loss = loss(X, Y, w, b)\n",
    "        print(\"Iteration %4d => Loss: %.6f\" % (i, current_loss))\n",
    "        if loss(X, Y, w - lr, b) < current_loss:\n",
    "              w -= lr\n",
    "        elif loss(X, Y, w + lr, b) < current_loss:\n",
    "               w += lr\n",
    "        elif loss(X, Y, w, b - lr) < current_loss:\n",
    "                b -= lr\n",
    "        elif loss(X, Y, w, b + lr) < current_loss:\n",
    "                b += lr\n",
    "        else:\n",
    "                return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0 => Loss: 812.866667\n",
      "Iteration    1 => Loss: 804.820547\n",
      "Iteration    2 => Loss: 796.818187\n",
      "Iteration    3 => Loss: 788.859587\n",
      "Iteration    4 => Loss: 780.944747\n",
      "Iteration    5 => Loss: 773.073667\n",
      "Iteration    6 => Loss: 765.246347\n",
      "Iteration    7 => Loss: 757.462787\n",
      "Iteration    8 => Loss: 749.722987\n",
      "Iteration    9 => Loss: 742.026947\n",
      "Iteration   10 => Loss: 734.374667\n",
      "Iteration   11 => Loss: 726.766147\n",
      "Iteration   12 => Loss: 719.201387\n",
      "Iteration   13 => Loss: 711.680387\n",
      "Iteration   14 => Loss: 704.203147\n",
      "Iteration   15 => Loss: 696.769667\n",
      "Iteration   16 => Loss: 689.379947\n",
      "Iteration   17 => Loss: 682.033987\n",
      "Iteration   18 => Loss: 674.731787\n",
      "Iteration   19 => Loss: 667.473347\n",
      "Iteration   20 => Loss: 660.258667\n",
      "Iteration   21 => Loss: 653.087747\n",
      "Iteration   22 => Loss: 645.960587\n",
      "Iteration   23 => Loss: 638.877187\n",
      "Iteration   24 => Loss: 631.837547\n",
      "Iteration   25 => Loss: 624.841667\n",
      "Iteration   26 => Loss: 617.889547\n",
      "Iteration   27 => Loss: 610.981187\n",
      "Iteration   28 => Loss: 604.116587\n",
      "Iteration   29 => Loss: 597.295747\n",
      "Iteration   30 => Loss: 590.518667\n",
      "Iteration   31 => Loss: 583.785347\n",
      "Iteration   32 => Loss: 577.095787\n",
      "Iteration   33 => Loss: 570.449987\n",
      "Iteration   34 => Loss: 563.847947\n",
      "Iteration   35 => Loss: 557.289667\n",
      "Iteration   36 => Loss: 550.775147\n",
      "Iteration   37 => Loss: 544.304387\n",
      "Iteration   38 => Loss: 537.877387\n",
      "Iteration   39 => Loss: 531.494147\n",
      "Iteration   40 => Loss: 525.154667\n",
      "Iteration   41 => Loss: 518.858947\n",
      "Iteration   42 => Loss: 512.606987\n",
      "Iteration   43 => Loss: 506.398787\n",
      "Iteration   44 => Loss: 500.234347\n",
      "Iteration   45 => Loss: 494.113667\n",
      "Iteration   46 => Loss: 488.036747\n",
      "Iteration   47 => Loss: 482.003587\n",
      "Iteration   48 => Loss: 476.014187\n",
      "Iteration   49 => Loss: 470.068547\n",
      "Iteration   50 => Loss: 464.166667\n",
      "Iteration   51 => Loss: 458.308547\n",
      "Iteration   52 => Loss: 452.494187\n",
      "Iteration   53 => Loss: 446.723587\n",
      "Iteration   54 => Loss: 440.996747\n",
      "Iteration   55 => Loss: 435.313667\n",
      "Iteration   56 => Loss: 429.674347\n",
      "Iteration   57 => Loss: 424.078787\n",
      "Iteration   58 => Loss: 418.526987\n",
      "Iteration   59 => Loss: 413.018947\n",
      "Iteration   60 => Loss: 407.554667\n",
      "Iteration   61 => Loss: 402.134147\n",
      "Iteration   62 => Loss: 396.757387\n",
      "Iteration   63 => Loss: 391.424387\n",
      "Iteration   64 => Loss: 386.135147\n",
      "Iteration   65 => Loss: 380.889667\n",
      "Iteration   66 => Loss: 375.687947\n",
      "Iteration   67 => Loss: 370.529987\n",
      "Iteration   68 => Loss: 365.415787\n",
      "Iteration   69 => Loss: 360.345347\n",
      "Iteration   70 => Loss: 355.318667\n",
      "Iteration   71 => Loss: 350.335747\n",
      "Iteration   72 => Loss: 345.396587\n",
      "Iteration   73 => Loss: 340.501187\n",
      "Iteration   74 => Loss: 335.649547\n",
      "Iteration   75 => Loss: 330.841667\n",
      "Iteration   76 => Loss: 326.077547\n",
      "Iteration   77 => Loss: 321.357187\n",
      "Iteration   78 => Loss: 316.680587\n",
      "Iteration   79 => Loss: 312.047747\n",
      "Iteration   80 => Loss: 307.458667\n",
      "Iteration   81 => Loss: 302.913347\n",
      "Iteration   82 => Loss: 298.411787\n",
      "Iteration   83 => Loss: 293.953987\n",
      "Iteration   84 => Loss: 289.539947\n",
      "Iteration   85 => Loss: 285.169667\n",
      "Iteration   86 => Loss: 280.843147\n",
      "Iteration   87 => Loss: 276.560387\n",
      "Iteration   88 => Loss: 272.321387\n",
      "Iteration   89 => Loss: 268.126147\n",
      "Iteration   90 => Loss: 263.974667\n",
      "Iteration   91 => Loss: 259.866947\n",
      "Iteration   92 => Loss: 255.802987\n",
      "Iteration   93 => Loss: 251.782787\n",
      "Iteration   94 => Loss: 247.806347\n",
      "Iteration   95 => Loss: 243.873667\n",
      "Iteration   96 => Loss: 239.984747\n",
      "Iteration   97 => Loss: 236.139587\n",
      "Iteration   98 => Loss: 232.338187\n",
      "Iteration   99 => Loss: 228.580547\n",
      "Iteration  100 => Loss: 224.866667\n",
      "Iteration  101 => Loss: 221.196547\n",
      "Iteration  102 => Loss: 217.570187\n",
      "Iteration  103 => Loss: 213.987587\n",
      "Iteration  104 => Loss: 210.448747\n",
      "Iteration  105 => Loss: 206.953667\n",
      "Iteration  106 => Loss: 203.502347\n",
      "Iteration  107 => Loss: 200.094787\n",
      "Iteration  108 => Loss: 196.730987\n",
      "Iteration  109 => Loss: 193.410947\n",
      "Iteration  110 => Loss: 190.134667\n",
      "Iteration  111 => Loss: 186.902147\n",
      "Iteration  112 => Loss: 183.713387\n",
      "Iteration  113 => Loss: 180.568387\n",
      "Iteration  114 => Loss: 177.467147\n",
      "Iteration  115 => Loss: 174.409667\n",
      "Iteration  116 => Loss: 171.395947\n",
      "Iteration  117 => Loss: 168.425987\n",
      "Iteration  118 => Loss: 165.499787\n",
      "Iteration  119 => Loss: 162.617347\n",
      "Iteration  120 => Loss: 159.778667\n",
      "Iteration  121 => Loss: 156.983747\n",
      "Iteration  122 => Loss: 154.232587\n",
      "Iteration  123 => Loss: 151.525187\n",
      "Iteration  124 => Loss: 148.861547\n",
      "Iteration  125 => Loss: 146.241667\n",
      "Iteration  126 => Loss: 143.665547\n",
      "Iteration  127 => Loss: 141.133187\n",
      "Iteration  128 => Loss: 138.644587\n",
      "Iteration  129 => Loss: 136.199747\n",
      "Iteration  130 => Loss: 133.798667\n",
      "Iteration  131 => Loss: 131.441347\n",
      "Iteration  132 => Loss: 129.127787\n",
      "Iteration  133 => Loss: 126.857987\n",
      "Iteration  134 => Loss: 124.631947\n",
      "Iteration  135 => Loss: 122.449667\n",
      "Iteration  136 => Loss: 120.311147\n",
      "Iteration  137 => Loss: 118.216387\n",
      "Iteration  138 => Loss: 116.165387\n",
      "Iteration  139 => Loss: 114.158147\n",
      "Iteration  140 => Loss: 112.194667\n",
      "Iteration  141 => Loss: 110.274947\n",
      "Iteration  142 => Loss: 108.398987\n",
      "Iteration  143 => Loss: 106.566787\n",
      "Iteration  144 => Loss: 104.778347\n",
      "Iteration  145 => Loss: 103.033667\n",
      "Iteration  146 => Loss: 101.332747\n",
      "Iteration  147 => Loss: 99.675587\n",
      "Iteration  148 => Loss: 98.062187\n",
      "Iteration  149 => Loss: 96.492547\n",
      "Iteration  150 => Loss: 94.966667\n",
      "Iteration  151 => Loss: 93.484547\n",
      "Iteration  152 => Loss: 92.046187\n",
      "Iteration  153 => Loss: 90.651587\n",
      "Iteration  154 => Loss: 89.300747\n",
      "Iteration  155 => Loss: 87.993667\n",
      "Iteration  156 => Loss: 86.730347\n",
      "Iteration  157 => Loss: 85.510787\n",
      "Iteration  158 => Loss: 84.334987\n",
      "Iteration  159 => Loss: 83.202947\n",
      "Iteration  160 => Loss: 82.114667\n",
      "Iteration  161 => Loss: 81.070147\n",
      "Iteration  162 => Loss: 80.069387\n",
      "Iteration  163 => Loss: 79.112387\n",
      "Iteration  164 => Loss: 78.199147\n",
      "Iteration  165 => Loss: 77.329667\n",
      "Iteration  166 => Loss: 76.503947\n",
      "Iteration  167 => Loss: 75.721987\n",
      "Iteration  168 => Loss: 74.983787\n",
      "Iteration  169 => Loss: 74.289347\n",
      "Iteration  170 => Loss: 73.638667\n",
      "Iteration  171 => Loss: 73.031747\n",
      "Iteration  172 => Loss: 72.468587\n",
      "Iteration  173 => Loss: 71.949187\n",
      "Iteration  174 => Loss: 71.473547\n",
      "Iteration  175 => Loss: 71.041667\n",
      "Iteration  176 => Loss: 70.653547\n",
      "Iteration  177 => Loss: 70.309187\n",
      "Iteration  178 => Loss: 70.008587\n",
      "Iteration  179 => Loss: 69.751747\n",
      "Iteration  180 => Loss: 69.538667\n",
      "Iteration  181 => Loss: 69.369347\n",
      "Iteration  182 => Loss: 69.243787\n",
      "Iteration  183 => Loss: 69.161987\n",
      "Iteration  184 => Loss: 69.123947\n",
      "Iteration  185 => Loss: 69.052847\n",
      "Iteration  186 => Loss: 68.981947\n",
      "Iteration  187 => Loss: 68.911247\n",
      "Iteration  188 => Loss: 68.840747\n",
      "Iteration  189 => Loss: 68.770447\n",
      "Iteration  190 => Loss: 68.700347\n",
      "Iteration  191 => Loss: 68.630447\n",
      "Iteration  192 => Loss: 68.560747\n",
      "Iteration  193 => Loss: 68.491247\n",
      "Iteration  194 => Loss: 68.421947\n",
      "Iteration  195 => Loss: 68.352847\n",
      "Iteration  196 => Loss: 68.283947\n",
      "Iteration  197 => Loss: 68.215247\n",
      "Iteration  198 => Loss: 68.146747\n",
      "Iteration  199 => Loss: 68.078447\n",
      "Iteration  200 => Loss: 68.010347\n",
      "Iteration  201 => Loss: 68.007853\n",
      "Iteration  202 => Loss: 67.937420\n",
      "Iteration  203 => Loss: 67.867187\n",
      "Iteration  204 => Loss: 67.797153\n",
      "Iteration  205 => Loss: 67.727320\n",
      "Iteration  206 => Loss: 67.657687\n",
      "Iteration  207 => Loss: 67.588253\n",
      "Iteration  208 => Loss: 67.519020\n",
      "Iteration  209 => Loss: 67.449987\n",
      "Iteration  210 => Loss: 67.381153\n",
      "Iteration  211 => Loss: 67.312520\n",
      "Iteration  212 => Loss: 67.244087\n",
      "Iteration  213 => Loss: 67.175853\n",
      "Iteration  214 => Loss: 67.107820\n",
      "Iteration  215 => Loss: 67.039987\n",
      "Iteration  216 => Loss: 66.972353\n",
      "Iteration  217 => Loss: 66.904920\n",
      "Iteration  218 => Loss: 66.837687\n",
      "Iteration  219 => Loss: 66.835887\n",
      "Iteration  220 => Loss: 66.766320\n",
      "Iteration  221 => Loss: 66.696953\n",
      "Iteration  222 => Loss: 66.627787\n",
      "Iteration  223 => Loss: 66.558820\n",
      "Iteration  224 => Loss: 66.490053\n",
      "Iteration  225 => Loss: 66.421487\n",
      "Iteration  226 => Loss: 66.353120\n",
      "Iteration  227 => Loss: 66.284953\n",
      "Iteration  228 => Loss: 66.216987\n",
      "Iteration  229 => Loss: 66.149220\n",
      "Iteration  230 => Loss: 66.081653\n",
      "Iteration  231 => Loss: 66.014287\n",
      "Iteration  232 => Loss: 65.947120\n",
      "Iteration  233 => Loss: 65.880153\n",
      "Iteration  234 => Loss: 65.813387\n",
      "Iteration  235 => Loss: 65.746820\n",
      "Iteration  236 => Loss: 65.680453\n",
      "Iteration  237 => Loss: 65.679347\n",
      "Iteration  238 => Loss: 65.610647\n",
      "Iteration  239 => Loss: 65.542147\n",
      "Iteration  240 => Loss: 65.473847\n",
      "Iteration  241 => Loss: 65.405747\n",
      "Iteration  242 => Loss: 65.337847\n",
      "Iteration  243 => Loss: 65.270147\n",
      "Iteration  244 => Loss: 65.202647\n",
      "Iteration  245 => Loss: 65.135347\n",
      "Iteration  246 => Loss: 65.068247\n",
      "Iteration  247 => Loss: 65.001347\n",
      "Iteration  248 => Loss: 64.934647\n",
      "Iteration  249 => Loss: 64.868147\n",
      "Iteration  250 => Loss: 64.801847\n",
      "Iteration  251 => Loss: 64.735747\n",
      "Iteration  252 => Loss: 64.669847\n",
      "Iteration  253 => Loss: 64.604147\n",
      "Iteration  254 => Loss: 64.538647\n",
      "Iteration  255 => Loss: 64.538233\n",
      "Iteration  256 => Loss: 64.470400\n",
      "Iteration  257 => Loss: 64.402767\n",
      "Iteration  258 => Loss: 64.335333\n",
      "Iteration  259 => Loss: 64.268100\n",
      "Iteration  260 => Loss: 64.201067\n",
      "Iteration  261 => Loss: 64.134233\n",
      "Iteration  262 => Loss: 64.067600\n",
      "Iteration  263 => Loss: 64.001167\n",
      "Iteration  264 => Loss: 63.934933\n",
      "Iteration  265 => Loss: 63.868900\n",
      "Iteration  266 => Loss: 63.803067\n",
      "Iteration  267 => Loss: 63.737433\n",
      "Iteration  268 => Loss: 63.672000\n",
      "Iteration  269 => Loss: 63.606767\n",
      "Iteration  270 => Loss: 63.541733\n",
      "Iteration  271 => Loss: 63.476900\n",
      "Iteration  272 => Loss: 63.412267\n",
      "Iteration  273 => Loss: 63.347833\n",
      "Iteration  274 => Loss: 63.345580\n",
      "Iteration  275 => Loss: 63.278813\n",
      "Iteration  276 => Loss: 63.212247\n",
      "Iteration  277 => Loss: 63.145880\n",
      "Iteration  278 => Loss: 63.079713\n",
      "Iteration  279 => Loss: 63.013747\n",
      "Iteration  280 => Loss: 62.947980\n",
      "Iteration  281 => Loss: 62.882413\n",
      "Iteration  282 => Loss: 62.817047\n",
      "Iteration  283 => Loss: 62.751880\n",
      "Iteration  284 => Loss: 62.686913\n",
      "Iteration  285 => Loss: 62.622147\n",
      "Iteration  286 => Loss: 62.557580\n",
      "Iteration  287 => Loss: 62.493213\n",
      "Iteration  288 => Loss: 62.429047\n",
      "Iteration  289 => Loss: 62.365080\n",
      "Iteration  290 => Loss: 62.301313\n",
      "Iteration  291 => Loss: 62.237747\n",
      "Iteration  292 => Loss: 62.236187\n",
      "Iteration  293 => Loss: 62.170287\n",
      "Iteration  294 => Loss: 62.104587\n",
      "Iteration  295 => Loss: 62.039087\n",
      "Iteration  296 => Loss: 61.973787\n",
      "Iteration  297 => Loss: 61.908687\n",
      "Iteration  298 => Loss: 61.843787\n",
      "Iteration  299 => Loss: 61.779087\n",
      "Iteration  300 => Loss: 61.714587\n",
      "Iteration  301 => Loss: 61.650287\n",
      "Iteration  302 => Loss: 61.586187\n",
      "Iteration  303 => Loss: 61.522287\n",
      "Iteration  304 => Loss: 61.458587\n",
      "Iteration  305 => Loss: 61.395087\n",
      "Iteration  306 => Loss: 61.331787\n",
      "Iteration  307 => Loss: 61.268687\n",
      "Iteration  308 => Loss: 61.205787\n",
      "Iteration  309 => Loss: 61.143087\n",
      "Iteration  310 => Loss: 61.142220\n",
      "Iteration  311 => Loss: 61.077187\n",
      "Iteration  312 => Loss: 61.012353\n",
      "Iteration  313 => Loss: 60.947720\n",
      "Iteration  314 => Loss: 60.883287\n",
      "Iteration  315 => Loss: 60.819053\n",
      "Iteration  316 => Loss: 60.755020\n",
      "Iteration  317 => Loss: 60.691187\n",
      "Iteration  318 => Loss: 60.627553\n",
      "Iteration  319 => Loss: 60.564120\n",
      "Iteration  320 => Loss: 60.500887\n",
      "Iteration  321 => Loss: 60.437853\n",
      "Iteration  322 => Loss: 60.375020\n",
      "Iteration  323 => Loss: 60.312387\n",
      "Iteration  324 => Loss: 60.249953\n",
      "Iteration  325 => Loss: 60.187720\n",
      "Iteration  326 => Loss: 60.125687\n",
      "Iteration  327 => Loss: 60.063853\n",
      "Iteration  328 => Loss: 60.063680\n",
      "Iteration  329 => Loss: 59.999513\n",
      "Iteration  330 => Loss: 59.935547\n",
      "Iteration  331 => Loss: 59.871780\n",
      "Iteration  332 => Loss: 59.808213\n",
      "Iteration  333 => Loss: 59.744847\n",
      "Iteration  334 => Loss: 59.681680\n",
      "Iteration  335 => Loss: 59.618713\n",
      "Iteration  336 => Loss: 59.555947\n",
      "Iteration  337 => Loss: 59.493380\n",
      "Iteration  338 => Loss: 59.431013\n",
      "Iteration  339 => Loss: 59.368847\n",
      "Iteration  340 => Loss: 59.306880\n",
      "Iteration  341 => Loss: 59.245113\n",
      "Iteration  342 => Loss: 59.183547\n",
      "Iteration  343 => Loss: 59.122180\n",
      "Iteration  344 => Loss: 59.061013\n",
      "Iteration  345 => Loss: 59.000047\n",
      "Iteration  346 => Loss: 58.939280\n",
      "Iteration  347 => Loss: 58.937267\n",
      "Iteration  348 => Loss: 58.874167\n",
      "Iteration  349 => Loss: 58.811267\n",
      "Iteration  350 => Loss: 58.748567\n",
      "Iteration  351 => Loss: 58.686067\n",
      "Iteration  352 => Loss: 58.623767\n",
      "Iteration  353 => Loss: 58.561667\n",
      "Iteration  354 => Loss: 58.499767\n",
      "Iteration  355 => Loss: 58.438067\n",
      "Iteration  356 => Loss: 58.376567\n",
      "Iteration  357 => Loss: 58.315267\n",
      "Iteration  358 => Loss: 58.254167\n",
      "Iteration  359 => Loss: 58.193267\n",
      "Iteration  360 => Loss: 58.132567\n",
      "Iteration  361 => Loss: 58.072067\n",
      "Iteration  362 => Loss: 58.011767\n",
      "Iteration  363 => Loss: 57.951667\n",
      "Iteration  364 => Loss: 57.891767\n",
      "Iteration  365 => Loss: 57.890447\n",
      "Iteration  366 => Loss: 57.828213\n",
      "Iteration  367 => Loss: 57.766180\n",
      "Iteration  368 => Loss: 57.704347\n",
      "Iteration  369 => Loss: 57.642713\n",
      "Iteration  370 => Loss: 57.581280\n",
      "Iteration  371 => Loss: 57.520047\n",
      "Iteration  372 => Loss: 57.459013\n",
      "Iteration  373 => Loss: 57.398180\n",
      "Iteration  374 => Loss: 57.337547\n",
      "Iteration  375 => Loss: 57.277113\n",
      "Iteration  376 => Loss: 57.216880\n",
      "Iteration  377 => Loss: 57.156847\n",
      "Iteration  378 => Loss: 57.097013\n",
      "Iteration  379 => Loss: 57.037380\n",
      "Iteration  380 => Loss: 56.977947\n",
      "Iteration  381 => Loss: 56.918713\n",
      "Iteration  382 => Loss: 56.859680\n",
      "Iteration  383 => Loss: 56.859053\n",
      "Iteration  384 => Loss: 56.797687\n",
      "Iteration  385 => Loss: 56.736520\n",
      "Iteration  386 => Loss: 56.675553\n",
      "Iteration  387 => Loss: 56.614787\n",
      "Iteration  388 => Loss: 56.554220\n",
      "Iteration  389 => Loss: 56.493853\n",
      "Iteration  390 => Loss: 56.433687\n",
      "Iteration  391 => Loss: 56.373720\n",
      "Iteration  392 => Loss: 56.313953\n",
      "Iteration  393 => Loss: 56.254387\n",
      "Iteration  394 => Loss: 56.195020\n",
      "Iteration  395 => Loss: 56.135853\n",
      "Iteration  396 => Loss: 56.076887\n",
      "Iteration  397 => Loss: 56.018120\n",
      "Iteration  398 => Loss: 55.959553\n",
      "Iteration  399 => Loss: 55.901187\n",
      "Iteration  400 => Loss: 55.843020\n",
      "Iteration  401 => Loss: 55.785053\n",
      "Iteration  402 => Loss: 55.782587\n",
      "Iteration  403 => Loss: 55.722287\n",
      "Iteration  404 => Loss: 55.662187\n",
      "Iteration  405 => Loss: 55.602287\n",
      "Iteration  406 => Loss: 55.542587\n",
      "Iteration  407 => Loss: 55.483087\n",
      "Iteration  408 => Loss: 55.423787\n",
      "Iteration  409 => Loss: 55.364687\n",
      "Iteration  410 => Loss: 55.305787\n",
      "Iteration  411 => Loss: 55.247087\n",
      "Iteration  412 => Loss: 55.188587\n",
      "Iteration  413 => Loss: 55.130287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  414 => Loss: 55.072187\n",
      "Iteration  415 => Loss: 55.014287\n",
      "Iteration  416 => Loss: 54.956587\n",
      "Iteration  417 => Loss: 54.899087\n",
      "Iteration  418 => Loss: 54.841787\n",
      "Iteration  419 => Loss: 54.784687\n",
      "Iteration  420 => Loss: 54.782913\n",
      "Iteration  421 => Loss: 54.723480\n",
      "Iteration  422 => Loss: 54.664247\n",
      "Iteration  423 => Loss: 54.605213\n",
      "Iteration  424 => Loss: 54.546380\n",
      "Iteration  425 => Loss: 54.487747\n",
      "Iteration  426 => Loss: 54.429313\n",
      "Iteration  427 => Loss: 54.371080\n",
      "Iteration  428 => Loss: 54.313047\n",
      "Iteration  429 => Loss: 54.255213\n",
      "Iteration  430 => Loss: 54.197580\n",
      "Iteration  431 => Loss: 54.140147\n",
      "Iteration  432 => Loss: 54.082913\n",
      "Iteration  433 => Loss: 54.025880\n",
      "Iteration  434 => Loss: 53.969047\n",
      "Iteration  435 => Loss: 53.912413\n",
      "Iteration  436 => Loss: 53.855980\n",
      "Iteration  437 => Loss: 53.799747\n",
      "Iteration  438 => Loss: 53.798667\n",
      "Iteration  439 => Loss: 53.740100\n",
      "Iteration  440 => Loss: 53.681733\n",
      "Iteration  441 => Loss: 53.623567\n",
      "Iteration  442 => Loss: 53.565600\n",
      "Iteration  443 => Loss: 53.507833\n",
      "Iteration  444 => Loss: 53.450267\n",
      "Iteration  445 => Loss: 53.392900\n",
      "Iteration  446 => Loss: 53.335733\n",
      "Iteration  447 => Loss: 53.278767\n",
      "Iteration  448 => Loss: 53.222000\n",
      "Iteration  449 => Loss: 53.165433\n",
      "Iteration  450 => Loss: 53.109067\n",
      "Iteration  451 => Loss: 53.052900\n",
      "Iteration  452 => Loss: 52.996933\n",
      "Iteration  453 => Loss: 52.941167\n",
      "Iteration  454 => Loss: 52.885600\n",
      "Iteration  455 => Loss: 52.830233\n",
      "Iteration  456 => Loss: 52.829847\n",
      "Iteration  457 => Loss: 52.772147\n",
      "Iteration  458 => Loss: 52.714647\n",
      "Iteration  459 => Loss: 52.657347\n",
      "Iteration  460 => Loss: 52.600247\n",
      "Iteration  461 => Loss: 52.543347\n",
      "Iteration  462 => Loss: 52.486647\n",
      "Iteration  463 => Loss: 52.430147\n",
      "Iteration  464 => Loss: 52.373847\n",
      "Iteration  465 => Loss: 52.317747\n",
      "Iteration  466 => Loss: 52.261847\n",
      "Iteration  467 => Loss: 52.206147\n",
      "Iteration  468 => Loss: 52.150647\n",
      "Iteration  469 => Loss: 52.095347\n",
      "Iteration  470 => Loss: 52.040247\n",
      "Iteration  471 => Loss: 51.985347\n",
      "Iteration  472 => Loss: 51.930647\n",
      "Iteration  473 => Loss: 51.876147\n",
      "Iteration  474 => Loss: 51.821847\n",
      "Iteration  475 => Loss: 51.819620\n",
      "Iteration  476 => Loss: 51.762987\n",
      "Iteration  477 => Loss: 51.706553\n",
      "Iteration  478 => Loss: 51.650320\n",
      "Iteration  479 => Loss: 51.594287\n",
      "Iteration  480 => Loss: 51.538453\n",
      "Iteration  481 => Loss: 51.482820\n",
      "Iteration  482 => Loss: 51.427387\n",
      "Iteration  483 => Loss: 51.372153\n",
      "Iteration  484 => Loss: 51.317120\n",
      "Iteration  485 => Loss: 51.262287\n",
      "Iteration  486 => Loss: 51.207653\n",
      "Iteration  487 => Loss: 51.153220\n",
      "Iteration  488 => Loss: 51.098987\n",
      "Iteration  489 => Loss: 51.044953\n",
      "Iteration  490 => Loss: 50.991120\n",
      "Iteration  491 => Loss: 50.937487\n",
      "Iteration  492 => Loss: 50.884053\n",
      "Iteration  493 => Loss: 50.882520\n",
      "Iteration  494 => Loss: 50.826753\n",
      "Iteration  495 => Loss: 50.771187\n",
      "Iteration  496 => Loss: 50.715820\n",
      "Iteration  497 => Loss: 50.660653\n",
      "Iteration  498 => Loss: 50.605687\n",
      "Iteration  499 => Loss: 50.550920\n",
      "Iteration  500 => Loss: 50.496353\n",
      "Iteration  501 => Loss: 50.441987\n",
      "Iteration  502 => Loss: 50.387820\n",
      "Iteration  503 => Loss: 50.333853\n",
      "Iteration  504 => Loss: 50.280087\n",
      "Iteration  505 => Loss: 50.226520\n",
      "Iteration  506 => Loss: 50.173153\n",
      "Iteration  507 => Loss: 50.119987\n",
      "Iteration  508 => Loss: 50.067020\n",
      "Iteration  509 => Loss: 50.014253\n",
      "Iteration  510 => Loss: 49.961687\n",
      "Iteration  511 => Loss: 49.960847\n",
      "Iteration  512 => Loss: 49.905947\n",
      "Iteration  513 => Loss: 49.851247\n",
      "Iteration  514 => Loss: 49.796747\n",
      "Iteration  515 => Loss: 49.742447\n",
      "Iteration  516 => Loss: 49.688347\n",
      "Iteration  517 => Loss: 49.634447\n",
      "Iteration  518 => Loss: 49.580747\n",
      "Iteration  519 => Loss: 49.527247\n",
      "Iteration  520 => Loss: 49.473947\n",
      "Iteration  521 => Loss: 49.420847\n",
      "Iteration  522 => Loss: 49.367947\n",
      "Iteration  523 => Loss: 49.315247\n",
      "Iteration  524 => Loss: 49.262747\n",
      "Iteration  525 => Loss: 49.210447\n",
      "Iteration  526 => Loss: 49.158347\n",
      "Iteration  527 => Loss: 49.106447\n",
      "Iteration  528 => Loss: 49.054747\n",
      "Iteration  529 => Loss: 49.054600\n",
      "Iteration  530 => Loss: 49.000567\n",
      "Iteration  531 => Loss: 48.946733\n",
      "Iteration  532 => Loss: 48.893100\n",
      "Iteration  533 => Loss: 48.839667\n",
      "Iteration  534 => Loss: 48.786433\n",
      "Iteration  535 => Loss: 48.733400\n",
      "Iteration  536 => Loss: 48.680567\n",
      "Iteration  537 => Loss: 48.627933\n",
      "Iteration  538 => Loss: 48.575500\n",
      "Iteration  539 => Loss: 48.523267\n",
      "Iteration  540 => Loss: 48.471233\n",
      "Iteration  541 => Loss: 48.419400\n",
      "Iteration  542 => Loss: 48.367767\n",
      "Iteration  543 => Loss: 48.316333\n",
      "Iteration  544 => Loss: 48.265100\n",
      "Iteration  545 => Loss: 48.214067\n",
      "Iteration  546 => Loss: 48.163233\n",
      "Iteration  547 => Loss: 48.112600\n",
      "Iteration  548 => Loss: 48.110613\n",
      "Iteration  549 => Loss: 48.057647\n",
      "Iteration  550 => Loss: 48.004880\n",
      "Iteration  551 => Loss: 47.952313\n",
      "Iteration  552 => Loss: 47.899947\n",
      "Iteration  553 => Loss: 47.847780\n",
      "Iteration  554 => Loss: 47.795813\n",
      "Iteration  555 => Loss: 47.744047\n",
      "Iteration  556 => Loss: 47.692480\n",
      "Iteration  557 => Loss: 47.641113\n",
      "Iteration  558 => Loss: 47.589947\n",
      "Iteration  559 => Loss: 47.538980\n",
      "Iteration  560 => Loss: 47.488213\n",
      "Iteration  561 => Loss: 47.437647\n",
      "Iteration  562 => Loss: 47.387280\n",
      "Iteration  563 => Loss: 47.337113\n",
      "Iteration  564 => Loss: 47.287147\n",
      "Iteration  565 => Loss: 47.237380\n",
      "Iteration  566 => Loss: 47.236087\n",
      "Iteration  567 => Loss: 47.183987\n",
      "Iteration  568 => Loss: 47.132087\n",
      "Iteration  569 => Loss: 47.080387\n",
      "Iteration  570 => Loss: 47.028887\n",
      "Iteration  571 => Loss: 46.977587\n",
      "Iteration  572 => Loss: 46.926487\n",
      "Iteration  573 => Loss: 46.875587\n",
      "Iteration  574 => Loss: 46.824887\n",
      "Iteration  575 => Loss: 46.774387\n",
      "Iteration  576 => Loss: 46.724087\n",
      "Iteration  577 => Loss: 46.673987\n",
      "Iteration  578 => Loss: 46.624087\n",
      "Iteration  579 => Loss: 46.574387\n",
      "Iteration  580 => Loss: 46.524887\n",
      "Iteration  581 => Loss: 46.475587\n",
      "Iteration  582 => Loss: 46.426487\n",
      "Iteration  583 => Loss: 46.377587\n",
      "Iteration  584 => Loss: 46.376987\n",
      "Iteration  585 => Loss: 46.325753\n",
      "Iteration  586 => Loss: 46.274720\n",
      "Iteration  587 => Loss: 46.223887\n",
      "Iteration  588 => Loss: 46.173253\n",
      "Iteration  589 => Loss: 46.122820\n",
      "Iteration  590 => Loss: 46.072587\n",
      "Iteration  591 => Loss: 46.022553\n",
      "Iteration  592 => Loss: 45.972720\n",
      "Iteration  593 => Loss: 45.923087\n",
      "Iteration  594 => Loss: 45.873653\n",
      "Iteration  595 => Loss: 45.824420\n",
      "Iteration  596 => Loss: 45.775387\n",
      "Iteration  597 => Loss: 45.726553\n",
      "Iteration  598 => Loss: 45.677920\n",
      "Iteration  599 => Loss: 45.629487\n",
      "Iteration  600 => Loss: 45.581253\n",
      "Iteration  601 => Loss: 45.533220\n",
      "Iteration  602 => Loss: 45.485387\n",
      "Iteration  603 => Loss: 45.482947\n",
      "Iteration  604 => Loss: 45.432780\n",
      "Iteration  605 => Loss: 45.382813\n",
      "Iteration  606 => Loss: 45.333047\n",
      "Iteration  607 => Loss: 45.283480\n",
      "Iteration  608 => Loss: 45.234113\n",
      "Iteration  609 => Loss: 45.184947\n",
      "Iteration  610 => Loss: 45.135980\n",
      "Iteration  611 => Loss: 45.087213\n",
      "Iteration  612 => Loss: 45.038647\n",
      "Iteration  613 => Loss: 44.990280\n",
      "Iteration  614 => Loss: 44.942113\n",
      "Iteration  615 => Loss: 44.894147\n",
      "Iteration  616 => Loss: 44.846380\n",
      "Iteration  617 => Loss: 44.798813\n",
      "Iteration  618 => Loss: 44.751447\n",
      "Iteration  619 => Loss: 44.704280\n",
      "Iteration  620 => Loss: 44.657313\n",
      "Iteration  621 => Loss: 44.655567\n",
      "Iteration  622 => Loss: 44.606267\n",
      "Iteration  623 => Loss: 44.557167\n",
      "Iteration  624 => Loss: 44.508267\n",
      "Iteration  625 => Loss: 44.459567\n",
      "Iteration  626 => Loss: 44.411067\n",
      "Iteration  627 => Loss: 44.362767\n",
      "Iteration  628 => Loss: 44.314667\n",
      "Iteration  629 => Loss: 44.266767\n",
      "Iteration  630 => Loss: 44.219067\n",
      "Iteration  631 => Loss: 44.171567\n",
      "Iteration  632 => Loss: 44.124267\n",
      "Iteration  633 => Loss: 44.077167\n",
      "Iteration  634 => Loss: 44.030267\n",
      "Iteration  635 => Loss: 43.983567\n",
      "Iteration  636 => Loss: 43.937067\n",
      "Iteration  637 => Loss: 43.890767\n",
      "Iteration  638 => Loss: 43.844667\n",
      "Iteration  639 => Loss: 43.843613\n",
      "Iteration  640 => Loss: 43.795180\n",
      "Iteration  641 => Loss: 43.746947\n",
      "Iteration  642 => Loss: 43.698913\n",
      "Iteration  643 => Loss: 43.651080\n",
      "Iteration  644 => Loss: 43.603447\n",
      "Iteration  645 => Loss: 43.556013\n",
      "Iteration  646 => Loss: 43.508780\n",
      "Iteration  647 => Loss: 43.461747\n",
      "Iteration  648 => Loss: 43.414913\n",
      "Iteration  649 => Loss: 43.368280\n",
      "Iteration  650 => Loss: 43.321847\n",
      "Iteration  651 => Loss: 43.275613\n",
      "Iteration  652 => Loss: 43.229580\n",
      "Iteration  653 => Loss: 43.183747\n",
      "Iteration  654 => Loss: 43.138113\n",
      "Iteration  655 => Loss: 43.092680\n",
      "Iteration  656 => Loss: 43.047447\n",
      "Iteration  657 => Loss: 43.047087\n",
      "Iteration  658 => Loss: 42.999520\n",
      "Iteration  659 => Loss: 42.952153\n",
      "Iteration  660 => Loss: 42.904987\n",
      "Iteration  661 => Loss: 42.858020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  662 => Loss: 42.811253\n",
      "Iteration  663 => Loss: 42.764687\n",
      "Iteration  664 => Loss: 42.718320\n",
      "Iteration  665 => Loss: 42.672153\n",
      "Iteration  666 => Loss: 42.626187\n",
      "Iteration  667 => Loss: 42.580420\n",
      "Iteration  668 => Loss: 42.534853\n",
      "Iteration  669 => Loss: 42.489487\n",
      "Iteration  670 => Loss: 42.444320\n",
      "Iteration  671 => Loss: 42.399353\n",
      "Iteration  672 => Loss: 42.354587\n",
      "Iteration  673 => Loss: 42.310020\n",
      "Iteration  674 => Loss: 42.265653\n",
      "Iteration  675 => Loss: 42.221487\n",
      "Iteration  676 => Loss: 42.219287\n",
      "Iteration  677 => Loss: 42.172787\n",
      "Iteration  678 => Loss: 42.126487\n",
      "Iteration  679 => Loss: 42.080387\n",
      "Iteration  680 => Loss: 42.034487\n",
      "Iteration  681 => Loss: 41.988787\n",
      "Iteration  682 => Loss: 41.943287\n",
      "Iteration  683 => Loss: 41.897987\n",
      "Iteration  684 => Loss: 41.852887\n",
      "Iteration  685 => Loss: 41.807987\n",
      "Iteration  686 => Loss: 41.763287\n",
      "Iteration  687 => Loss: 41.718787\n",
      "Iteration  688 => Loss: 41.674487\n",
      "Iteration  689 => Loss: 41.630387\n",
      "Iteration  690 => Loss: 41.586487\n",
      "Iteration  691 => Loss: 41.542787\n",
      "Iteration  692 => Loss: 41.499287\n",
      "Iteration  693 => Loss: 41.455987\n",
      "Iteration  694 => Loss: 41.454480\n",
      "Iteration  695 => Loss: 41.408847\n",
      "Iteration  696 => Loss: 41.363413\n",
      "Iteration  697 => Loss: 41.318180\n",
      "Iteration  698 => Loss: 41.273147\n",
      "Iteration  699 => Loss: 41.228313\n",
      "Iteration  700 => Loss: 41.183680\n",
      "Iteration  701 => Loss: 41.139247\n",
      "Iteration  702 => Loss: 41.095013\n",
      "Iteration  703 => Loss: 41.050980\n",
      "Iteration  704 => Loss: 41.007147\n",
      "Iteration  705 => Loss: 40.963513\n",
      "Iteration  706 => Loss: 40.920080\n",
      "Iteration  707 => Loss: 40.876847\n",
      "Iteration  708 => Loss: 40.833813\n",
      "Iteration  709 => Loss: 40.790980\n",
      "Iteration  710 => Loss: 40.748347\n",
      "Iteration  711 => Loss: 40.705913\n",
      "Iteration  712 => Loss: 40.705100\n",
      "Iteration  713 => Loss: 40.660333\n",
      "Iteration  714 => Loss: 40.615767\n",
      "Iteration  715 => Loss: 40.571400\n",
      "Iteration  716 => Loss: 40.527233\n",
      "Iteration  717 => Loss: 40.483267\n",
      "Iteration  718 => Loss: 40.439500\n",
      "Iteration  719 => Loss: 40.395933\n",
      "Iteration  720 => Loss: 40.352567\n",
      "Iteration  721 => Loss: 40.309400\n",
      "Iteration  722 => Loss: 40.266433\n",
      "Iteration  723 => Loss: 40.223667\n",
      "Iteration  724 => Loss: 40.181100\n",
      "Iteration  725 => Loss: 40.138733\n",
      "Iteration  726 => Loss: 40.096567\n",
      "Iteration  727 => Loss: 40.054600\n",
      "Iteration  728 => Loss: 40.012833\n",
      "Iteration  729 => Loss: 39.971267\n",
      "Iteration  730 => Loss: 39.971147\n",
      "Iteration  731 => Loss: 39.927247\n",
      "Iteration  732 => Loss: 39.883547\n",
      "Iteration  733 => Loss: 39.840047\n",
      "Iteration  734 => Loss: 39.796747\n",
      "Iteration  735 => Loss: 39.753647\n",
      "Iteration  736 => Loss: 39.710747\n",
      "Iteration  737 => Loss: 39.668047\n",
      "Iteration  738 => Loss: 39.625547\n",
      "Iteration  739 => Loss: 39.583247\n",
      "Iteration  740 => Loss: 39.541147\n",
      "Iteration  741 => Loss: 39.499247\n",
      "Iteration  742 => Loss: 39.457547\n",
      "Iteration  743 => Loss: 39.416047\n",
      "Iteration  744 => Loss: 39.374747\n",
      "Iteration  745 => Loss: 39.333647\n",
      "Iteration  746 => Loss: 39.292747\n",
      "Iteration  747 => Loss: 39.252047\n",
      "Iteration  748 => Loss: 39.211547\n",
      "Iteration  749 => Loss: 39.209587\n",
      "Iteration  750 => Loss: 39.166753\n",
      "Iteration  751 => Loss: 39.124120\n",
      "Iteration  752 => Loss: 39.081687\n",
      "Iteration  753 => Loss: 39.039453\n",
      "Iteration  754 => Loss: 38.997420\n",
      "Iteration  755 => Loss: 38.955587\n",
      "Iteration  756 => Loss: 38.913953\n",
      "Iteration  757 => Loss: 38.872520\n",
      "Iteration  758 => Loss: 38.831287\n",
      "Iteration  759 => Loss: 38.790253\n",
      "Iteration  760 => Loss: 38.749420\n",
      "Iteration  761 => Loss: 38.708787\n",
      "Iteration  762 => Loss: 38.668353\n",
      "Iteration  763 => Loss: 38.628120\n",
      "Iteration  764 => Loss: 38.588087\n",
      "Iteration  765 => Loss: 38.548253\n",
      "Iteration  766 => Loss: 38.508620\n",
      "Iteration  767 => Loss: 38.507353\n",
      "Iteration  768 => Loss: 38.465387\n",
      "Iteration  769 => Loss: 38.423620\n",
      "Iteration  770 => Loss: 38.382053\n",
      "Iteration  771 => Loss: 38.340687\n",
      "Iteration  772 => Loss: 38.299520\n",
      "Iteration  773 => Loss: 38.258553\n",
      "Iteration  774 => Loss: 38.217787\n",
      "Iteration  775 => Loss: 38.177220\n",
      "Iteration  776 => Loss: 38.136853\n",
      "Iteration  777 => Loss: 38.096687\n",
      "Iteration  778 => Loss: 38.056720\n",
      "Iteration  779 => Loss: 38.016953\n",
      "Iteration  780 => Loss: 37.977387\n",
      "Iteration  781 => Loss: 37.938020\n",
      "Iteration  782 => Loss: 37.898853\n",
      "Iteration  783 => Loss: 37.859887\n",
      "Iteration  784 => Loss: 37.821120\n",
      "Iteration  785 => Loss: 37.820547\n",
      "Iteration  786 => Loss: 37.779447\n",
      "Iteration  787 => Loss: 37.738547\n",
      "Iteration  788 => Loss: 37.697847\n",
      "Iteration  789 => Loss: 37.657347\n",
      "Iteration  790 => Loss: 37.617047\n",
      "Iteration  791 => Loss: 37.576947\n",
      "Iteration  792 => Loss: 37.537047\n",
      "Iteration  793 => Loss: 37.497347\n",
      "Iteration  794 => Loss: 37.457847\n",
      "Iteration  795 => Loss: 37.418547\n",
      "Iteration  796 => Loss: 37.379447\n",
      "Iteration  797 => Loss: 37.340547\n",
      "Iteration  798 => Loss: 37.301847\n",
      "Iteration  799 => Loss: 37.263347\n",
      "Iteration  800 => Loss: 37.225047\n",
      "Iteration  801 => Loss: 37.186947\n",
      "Iteration  802 => Loss: 37.149047\n",
      "Iteration  803 => Loss: 37.111347\n",
      "Iteration  804 => Loss: 37.108933\n",
      "Iteration  805 => Loss: 37.068900\n",
      "Iteration  806 => Loss: 37.029067\n",
      "Iteration  807 => Loss: 36.989433\n",
      "Iteration  808 => Loss: 36.950000\n",
      "Iteration  809 => Loss: 36.910767\n",
      "Iteration  810 => Loss: 36.871733\n",
      "Iteration  811 => Loss: 36.832900\n",
      "Iteration  812 => Loss: 36.794267\n",
      "Iteration  813 => Loss: 36.755833\n",
      "Iteration  814 => Loss: 36.717600\n",
      "Iteration  815 => Loss: 36.679567\n",
      "Iteration  816 => Loss: 36.641733\n",
      "Iteration  817 => Loss: 36.604100\n",
      "Iteration  818 => Loss: 36.566667\n",
      "Iteration  819 => Loss: 36.529433\n",
      "Iteration  820 => Loss: 36.492400\n",
      "Iteration  821 => Loss: 36.455567\n",
      "Iteration  822 => Loss: 36.453847\n",
      "Iteration  823 => Loss: 36.414680\n",
      "Iteration  824 => Loss: 36.375713\n",
      "Iteration  825 => Loss: 36.336947\n",
      "Iteration  826 => Loss: 36.298380\n",
      "Iteration  827 => Loss: 36.260013\n",
      "Iteration  828 => Loss: 36.221847\n",
      "Iteration  829 => Loss: 36.183880\n",
      "Iteration  830 => Loss: 36.146113\n",
      "Iteration  831 => Loss: 36.108547\n",
      "Iteration  832 => Loss: 36.071180\n",
      "Iteration  833 => Loss: 36.034013\n",
      "Iteration  834 => Loss: 35.997047\n",
      "Iteration  835 => Loss: 35.960280\n",
      "Iteration  836 => Loss: 35.923713\n",
      "Iteration  837 => Loss: 35.887347\n",
      "Iteration  838 => Loss: 35.851180\n",
      "Iteration  839 => Loss: 35.815213\n",
      "Iteration  840 => Loss: 35.814187\n",
      "Iteration  841 => Loss: 35.775887\n",
      "Iteration  842 => Loss: 35.737787\n",
      "Iteration  843 => Loss: 35.699887\n",
      "Iteration  844 => Loss: 35.662187\n",
      "Iteration  845 => Loss: 35.624687\n",
      "Iteration  846 => Loss: 35.587387\n",
      "Iteration  847 => Loss: 35.550287\n",
      "Iteration  848 => Loss: 35.513387\n",
      "Iteration  849 => Loss: 35.476687\n",
      "Iteration  850 => Loss: 35.440187\n",
      "Iteration  851 => Loss: 35.403887\n",
      "Iteration  852 => Loss: 35.367787\n",
      "Iteration  853 => Loss: 35.331887\n",
      "Iteration  854 => Loss: 35.296187\n",
      "Iteration  855 => Loss: 35.260687\n",
      "Iteration  856 => Loss: 35.225387\n",
      "Iteration  857 => Loss: 35.190287\n",
      "Iteration  858 => Loss: 35.189953\n",
      "Iteration  859 => Loss: 35.152520\n",
      "Iteration  860 => Loss: 35.115287\n",
      "Iteration  861 => Loss: 35.078253\n",
      "Iteration  862 => Loss: 35.041420\n",
      "Iteration  863 => Loss: 35.004787\n",
      "Iteration  864 => Loss: 34.968353\n",
      "Iteration  865 => Loss: 34.932120\n",
      "Iteration  866 => Loss: 34.896087\n",
      "Iteration  867 => Loss: 34.860253\n",
      "Iteration  868 => Loss: 34.824620\n",
      "Iteration  869 => Loss: 34.789187\n",
      "Iteration  870 => Loss: 34.753953\n",
      "Iteration  871 => Loss: 34.718920\n",
      "Iteration  872 => Loss: 34.684087\n",
      "Iteration  873 => Loss: 34.649453\n",
      "Iteration  874 => Loss: 34.615020\n",
      "Iteration  875 => Loss: 34.580787\n",
      "Iteration  876 => Loss: 34.546753\n",
      "Iteration  877 => Loss: 34.544580\n",
      "Iteration  878 => Loss: 34.508213\n",
      "Iteration  879 => Loss: 34.472047\n",
      "Iteration  880 => Loss: 34.436080\n",
      "Iteration  881 => Loss: 34.400313\n",
      "Iteration  882 => Loss: 34.364747\n",
      "Iteration  883 => Loss: 34.329380\n",
      "Iteration  884 => Loss: 34.294213\n",
      "Iteration  885 => Loss: 34.259247\n",
      "Iteration  886 => Loss: 34.224480\n",
      "Iteration  887 => Loss: 34.189913\n",
      "Iteration  888 => Loss: 34.155547\n",
      "Iteration  889 => Loss: 34.121380\n",
      "Iteration  890 => Loss: 34.087413\n",
      "Iteration  891 => Loss: 34.053647\n",
      "Iteration  892 => Loss: 34.020080\n",
      "Iteration  893 => Loss: 33.986713\n",
      "Iteration  894 => Loss: 33.953547\n",
      "Iteration  895 => Loss: 33.952067\n",
      "Iteration  896 => Loss: 33.916567\n",
      "Iteration  897 => Loss: 33.881267\n",
      "Iteration  898 => Loss: 33.846167\n",
      "Iteration  899 => Loss: 33.811267\n",
      "Iteration  900 => Loss: 33.776567\n",
      "Iteration  901 => Loss: 33.742067\n",
      "Iteration  902 => Loss: 33.707767\n",
      "Iteration  903 => Loss: 33.673667\n",
      "Iteration  904 => Loss: 33.639767\n",
      "Iteration  905 => Loss: 33.606067\n",
      "Iteration  906 => Loss: 33.572567\n",
      "Iteration  907 => Loss: 33.539267\n",
      "Iteration  908 => Loss: 33.506167\n",
      "Iteration  909 => Loss: 33.473267\n",
      "Iteration  910 => Loss: 33.440567\n",
      "Iteration  911 => Loss: 33.408067\n",
      "Iteration  912 => Loss: 33.375767\n",
      "Iteration  913 => Loss: 33.374980\n",
      "Iteration  914 => Loss: 33.340347\n",
      "Iteration  915 => Loss: 33.305913\n",
      "Iteration  916 => Loss: 33.271680\n",
      "Iteration  917 => Loss: 33.237647\n",
      "Iteration  918 => Loss: 33.203813\n",
      "Iteration  919 => Loss: 33.170180\n",
      "Iteration  920 => Loss: 33.136747\n",
      "Iteration  921 => Loss: 33.103513\n",
      "Iteration  922 => Loss: 33.070480\n",
      "Iteration  923 => Loss: 33.037647\n",
      "Iteration  924 => Loss: 33.005013\n",
      "Iteration  925 => Loss: 32.972580\n",
      "Iteration  926 => Loss: 32.940347\n",
      "Iteration  927 => Loss: 32.908313\n",
      "Iteration  928 => Loss: 32.876480\n",
      "Iteration  929 => Loss: 32.844847\n",
      "Iteration  930 => Loss: 32.813413\n",
      "Iteration  931 => Loss: 32.813320\n",
      "Iteration  932 => Loss: 32.779553\n",
      "Iteration  933 => Loss: 32.745987\n",
      "Iteration  934 => Loss: 32.712620\n",
      "Iteration  935 => Loss: 32.679453\n",
      "Iteration  936 => Loss: 32.646487\n",
      "Iteration  937 => Loss: 32.613720\n",
      "Iteration  938 => Loss: 32.581153\n",
      "Iteration  939 => Loss: 32.548787\n",
      "Iteration  940 => Loss: 32.516620\n",
      "Iteration  941 => Loss: 32.484653\n",
      "Iteration  942 => Loss: 32.452887\n",
      "Iteration  943 => Loss: 32.421320\n",
      "Iteration  944 => Loss: 32.389953\n",
      "Iteration  945 => Loss: 32.358787\n",
      "Iteration  946 => Loss: 32.327820\n",
      "Iteration  947 => Loss: 32.297053\n",
      "Iteration  948 => Loss: 32.266487\n",
      "Iteration  949 => Loss: 32.236120\n",
      "Iteration  950 => Loss: 32.234187\n",
      "Iteration  951 => Loss: 32.201487\n",
      "Iteration  952 => Loss: 32.168987\n",
      "Iteration  953 => Loss: 32.136687\n",
      "Iteration  954 => Loss: 32.104587\n",
      "Iteration  955 => Loss: 32.072687\n",
      "Iteration  956 => Loss: 32.040987\n",
      "Iteration  957 => Loss: 32.009487\n",
      "Iteration  958 => Loss: 31.978187\n",
      "Iteration  959 => Loss: 31.947087\n",
      "Iteration  960 => Loss: 31.916187\n",
      "Iteration  961 => Loss: 31.885487\n",
      "Iteration  962 => Loss: 31.854987\n",
      "Iteration  963 => Loss: 31.824687\n",
      "Iteration  964 => Loss: 31.794587\n",
      "Iteration  965 => Loss: 31.764687\n",
      "Iteration  966 => Loss: 31.734987\n",
      "Iteration  967 => Loss: 31.705487\n",
      "Iteration  968 => Loss: 31.704247\n",
      "Iteration  969 => Loss: 31.672413\n",
      "Iteration  970 => Loss: 31.640780\n",
      "Iteration  971 => Loss: 31.609347\n",
      "Iteration  972 => Loss: 31.578113\n",
      "Iteration  973 => Loss: 31.547080\n",
      "Iteration  974 => Loss: 31.516247\n",
      "Iteration  975 => Loss: 31.485613\n",
      "Iteration  976 => Loss: 31.455180\n",
      "Iteration  977 => Loss: 31.424947\n",
      "Iteration  978 => Loss: 31.394913\n",
      "Iteration  979 => Loss: 31.365080\n",
      "Iteration  980 => Loss: 31.335447\n",
      "Iteration  981 => Loss: 31.306013\n",
      "Iteration  982 => Loss: 31.276780\n",
      "Iteration  983 => Loss: 31.247747\n",
      "Iteration  984 => Loss: 31.218913\n",
      "Iteration  985 => Loss: 31.190280\n",
      "Iteration  986 => Loss: 31.189733\n",
      "Iteration  987 => Loss: 31.158767\n",
      "Iteration  988 => Loss: 31.128000\n",
      "Iteration  989 => Loss: 31.097433\n",
      "Iteration  990 => Loss: 31.067067\n",
      "Iteration  991 => Loss: 31.036900\n",
      "Iteration  992 => Loss: 31.006933\n",
      "Iteration  993 => Loss: 30.977167\n",
      "Iteration  994 => Loss: 30.947600\n",
      "Iteration  995 => Loss: 30.918233\n",
      "Iteration  996 => Loss: 30.889067\n",
      "Iteration  997 => Loss: 30.860100\n",
      "Iteration  998 => Loss: 30.831333\n",
      "Iteration  999 => Loss: 30.802767\n",
      "Iteration 1000 => Loss: 30.774400\n",
      "Iteration 1001 => Loss: 30.746233\n",
      "Iteration 1002 => Loss: 30.718267\n",
      "Iteration 1003 => Loss: 30.690500\n",
      "Iteration 1004 => Loss: 30.662933\n",
      "Iteration 1005 => Loss: 30.660547\n",
      "Iteration 1006 => Loss: 30.630647\n",
      "Iteration 1007 => Loss: 30.600947\n",
      "Iteration 1008 => Loss: 30.571447\n",
      "Iteration 1009 => Loss: 30.542147\n",
      "Iteration 1010 => Loss: 30.513047\n",
      "Iteration 1011 => Loss: 30.484147\n",
      "Iteration 1012 => Loss: 30.455447\n",
      "Iteration 1013 => Loss: 30.426947\n",
      "Iteration 1014 => Loss: 30.398647\n",
      "Iteration 1015 => Loss: 30.370547\n",
      "Iteration 1016 => Loss: 30.342647\n",
      "Iteration 1017 => Loss: 30.314947\n",
      "Iteration 1018 => Loss: 30.287447\n",
      "Iteration 1019 => Loss: 30.260147\n",
      "Iteration 1020 => Loss: 30.233047\n",
      "Iteration 1021 => Loss: 30.206147\n",
      "Iteration 1022 => Loss: 30.179447\n",
      "Iteration 1023 => Loss: 30.177753\n",
      "Iteration 1024 => Loss: 30.148720\n",
      "Iteration 1025 => Loss: 30.119887\n",
      "Iteration 1026 => Loss: 30.091253\n",
      "Iteration 1027 => Loss: 30.062820\n",
      "Iteration 1028 => Loss: 30.034587\n",
      "Iteration 1029 => Loss: 30.006553\n",
      "Iteration 1030 => Loss: 29.978720\n",
      "Iteration 1031 => Loss: 29.951087\n",
      "Iteration 1032 => Loss: 29.923653\n",
      "Iteration 1033 => Loss: 29.896420\n",
      "Iteration 1034 => Loss: 29.869387\n",
      "Iteration 1035 => Loss: 29.842553\n",
      "Iteration 1036 => Loss: 29.815920\n",
      "Iteration 1037 => Loss: 29.789487\n",
      "Iteration 1038 => Loss: 29.763253\n",
      "Iteration 1039 => Loss: 29.737220\n",
      "Iteration 1040 => Loss: 29.711387\n",
      "Iteration 1041 => Loss: 29.710387\n",
      "Iteration 1042 => Loss: 29.682220\n",
      "Iteration 1043 => Loss: 29.654253\n",
      "Iteration 1044 => Loss: 29.626487\n",
      "Iteration 1045 => Loss: 29.598920\n",
      "Iteration 1046 => Loss: 29.571553\n",
      "Iteration 1047 => Loss: 29.544387\n",
      "Iteration 1048 => Loss: 29.517420\n",
      "Iteration 1049 => Loss: 29.490653\n",
      "Iteration 1050 => Loss: 29.464087\n",
      "Iteration 1051 => Loss: 29.437720\n",
      "Iteration 1052 => Loss: 29.411553\n",
      "Iteration 1053 => Loss: 29.385587\n",
      "Iteration 1054 => Loss: 29.359820\n",
      "Iteration 1055 => Loss: 29.334253\n",
      "Iteration 1056 => Loss: 29.308887\n",
      "Iteration 1057 => Loss: 29.283720\n",
      "Iteration 1058 => Loss: 29.258753\n",
      "Iteration 1059 => Loss: 29.258447\n",
      "Iteration 1060 => Loss: 29.231147\n",
      "Iteration 1061 => Loss: 29.204047\n",
      "Iteration 1062 => Loss: 29.177147\n",
      "Iteration 1063 => Loss: 29.150447\n",
      "Iteration 1064 => Loss: 29.123947\n",
      "Iteration 1065 => Loss: 29.097647\n",
      "Iteration 1066 => Loss: 29.071547\n",
      "Iteration 1067 => Loss: 29.045647\n",
      "Iteration 1068 => Loss: 29.019947\n",
      "Iteration 1069 => Loss: 28.994447\n",
      "Iteration 1070 => Loss: 28.969147\n",
      "Iteration 1071 => Loss: 28.944047\n",
      "Iteration 1072 => Loss: 28.919147\n",
      "Iteration 1073 => Loss: 28.894447\n",
      "Iteration 1074 => Loss: 28.869947\n",
      "Iteration 1075 => Loss: 28.845647\n",
      "Iteration 1076 => Loss: 28.821547\n",
      "Iteration 1077 => Loss: 28.797647\n",
      "Iteration 1078 => Loss: 28.795500\n",
      "Iteration 1079 => Loss: 28.769267\n",
      "Iteration 1080 => Loss: 28.743233\n",
      "Iteration 1081 => Loss: 28.717400\n",
      "Iteration 1082 => Loss: 28.691767\n",
      "Iteration 1083 => Loss: 28.666333\n",
      "Iteration 1084 => Loss: 28.641100\n",
      "Iteration 1085 => Loss: 28.616067\n",
      "Iteration 1086 => Loss: 28.591233\n",
      "Iteration 1087 => Loss: 28.566600\n",
      "Iteration 1088 => Loss: 28.542167\n",
      "Iteration 1089 => Loss: 28.517933\n",
      "Iteration 1090 => Loss: 28.493900\n",
      "Iteration 1091 => Loss: 28.470067\n",
      "Iteration 1092 => Loss: 28.446433\n",
      "Iteration 1093 => Loss: 28.423000\n",
      "Iteration 1094 => Loss: 28.399767\n",
      "Iteration 1095 => Loss: 28.376733\n",
      "Iteration 1096 => Loss: 28.375280\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1097 => Loss: 28.349913\n",
      "Iteration 1098 => Loss: 28.324747\n",
      "Iteration 1099 => Loss: 28.299780\n",
      "Iteration 1100 => Loss: 28.275013\n",
      "Iteration 1101 => Loss: 28.250447\n",
      "Iteration 1102 => Loss: 28.226080\n",
      "Iteration 1103 => Loss: 28.201913\n",
      "Iteration 1104 => Loss: 28.177947\n",
      "Iteration 1105 => Loss: 28.154180\n",
      "Iteration 1106 => Loss: 28.130613\n",
      "Iteration 1107 => Loss: 28.107247\n",
      "Iteration 1108 => Loss: 28.084080\n",
      "Iteration 1109 => Loss: 28.061113\n",
      "Iteration 1110 => Loss: 28.038347\n",
      "Iteration 1111 => Loss: 28.015780\n",
      "Iteration 1112 => Loss: 27.993413\n",
      "Iteration 1113 => Loss: 27.971247\n",
      "Iteration 1114 => Loss: 27.970487\n",
      "Iteration 1115 => Loss: 27.945987\n",
      "Iteration 1116 => Loss: 27.921687\n",
      "Iteration 1117 => Loss: 27.897587\n",
      "Iteration 1118 => Loss: 27.873687\n",
      "Iteration 1119 => Loss: 27.849987\n",
      "Iteration 1120 => Loss: 27.826487\n",
      "Iteration 1121 => Loss: 27.803187\n",
      "Iteration 1122 => Loss: 27.780087\n",
      "Iteration 1123 => Loss: 27.757187\n",
      "Iteration 1124 => Loss: 27.734487\n",
      "Iteration 1125 => Loss: 27.711987\n",
      "Iteration 1126 => Loss: 27.689687\n",
      "Iteration 1127 => Loss: 27.667587\n",
      "Iteration 1128 => Loss: 27.645687\n",
      "Iteration 1129 => Loss: 27.623987\n",
      "Iteration 1130 => Loss: 27.602487\n",
      "Iteration 1131 => Loss: 27.581187\n",
      "Iteration 1132 => Loss: 27.581120\n",
      "Iteration 1133 => Loss: 27.557487\n",
      "Iteration 1134 => Loss: 27.534053\n",
      "Iteration 1135 => Loss: 27.510820\n",
      "Iteration 1136 => Loss: 27.487787\n",
      "Iteration 1137 => Loss: 27.464953\n",
      "Iteration 1138 => Loss: 27.442320\n",
      "Iteration 1139 => Loss: 27.419887\n",
      "Iteration 1140 => Loss: 27.397653\n",
      "Iteration 1141 => Loss: 27.375620\n",
      "Iteration 1142 => Loss: 27.353787\n",
      "Iteration 1143 => Loss: 27.332153\n",
      "Iteration 1144 => Loss: 27.310720\n",
      "Iteration 1145 => Loss: 27.289487\n",
      "Iteration 1146 => Loss: 27.268453\n",
      "Iteration 1147 => Loss: 27.247620\n",
      "Iteration 1148 => Loss: 27.226987\n",
      "Iteration 1149 => Loss: 27.206553\n",
      "Iteration 1150 => Loss: 27.186320\n",
      "Iteration 1151 => Loss: 27.184413\n",
      "Iteration 1152 => Loss: 27.161847\n",
      "Iteration 1153 => Loss: 27.139480\n",
      "Iteration 1154 => Loss: 27.117313\n",
      "Iteration 1155 => Loss: 27.095347\n",
      "Iteration 1156 => Loss: 27.073580\n",
      "Iteration 1157 => Loss: 27.052013\n",
      "Iteration 1158 => Loss: 27.030647\n",
      "Iteration 1159 => Loss: 27.009480\n",
      "Iteration 1160 => Loss: 26.988513\n",
      "Iteration 1161 => Loss: 26.967747\n",
      "Iteration 1162 => Loss: 26.947180\n",
      "Iteration 1163 => Loss: 26.926813\n",
      "Iteration 1164 => Loss: 26.906647\n",
      "Iteration 1165 => Loss: 26.886680\n",
      "Iteration 1166 => Loss: 26.866913\n",
      "Iteration 1167 => Loss: 26.847347\n",
      "Iteration 1168 => Loss: 26.827980\n",
      "Iteration 1169 => Loss: 26.826767\n",
      "Iteration 1170 => Loss: 26.805067\n",
      "Iteration 1171 => Loss: 26.783567\n",
      "Iteration 1172 => Loss: 26.762267\n",
      "Iteration 1173 => Loss: 26.741167\n",
      "Iteration 1174 => Loss: 26.720267\n",
      "Iteration 1175 => Loss: 26.699567\n",
      "Iteration 1176 => Loss: 26.679067\n",
      "Iteration 1177 => Loss: 26.658767\n",
      "Iteration 1178 => Loss: 26.638667\n",
      "Iteration 1179 => Loss: 26.618767\n",
      "Iteration 1180 => Loss: 26.599067\n",
      "Iteration 1181 => Loss: 26.579567\n",
      "Iteration 1182 => Loss: 26.560267\n",
      "Iteration 1183 => Loss: 26.541167\n",
      "Iteration 1184 => Loss: 26.522267\n",
      "Iteration 1185 => Loss: 26.503567\n",
      "Iteration 1186 => Loss: 26.485067\n",
      "Iteration 1187 => Loss: 26.484547\n",
      "Iteration 1188 => Loss: 26.463713\n",
      "Iteration 1189 => Loss: 26.443080\n",
      "Iteration 1190 => Loss: 26.422647\n",
      "Iteration 1191 => Loss: 26.402413\n",
      "Iteration 1192 => Loss: 26.382380\n",
      "Iteration 1193 => Loss: 26.362547\n",
      "Iteration 1194 => Loss: 26.342913\n",
      "Iteration 1195 => Loss: 26.323480\n",
      "Iteration 1196 => Loss: 26.304247\n",
      "Iteration 1197 => Loss: 26.285213\n",
      "Iteration 1198 => Loss: 26.266380\n",
      "Iteration 1199 => Loss: 26.247747\n",
      "Iteration 1200 => Loss: 26.229313\n",
      "Iteration 1201 => Loss: 26.211080\n",
      "Iteration 1202 => Loss: 26.193047\n",
      "Iteration 1203 => Loss: 26.175213\n",
      "Iteration 1204 => Loss: 26.157580\n",
      "Iteration 1205 => Loss: 26.140147\n",
      "Iteration 1206 => Loss: 26.137787\n",
      "Iteration 1207 => Loss: 26.118020\n",
      "Iteration 1208 => Loss: 26.098453\n",
      "Iteration 1209 => Loss: 26.079087\n",
      "Iteration 1210 => Loss: 26.059920\n",
      "Iteration 1211 => Loss: 26.040953\n",
      "Iteration 1212 => Loss: 26.022187\n",
      "Iteration 1213 => Loss: 26.003620\n",
      "Iteration 1214 => Loss: 25.985253\n",
      "Iteration 1215 => Loss: 25.967087\n",
      "Iteration 1216 => Loss: 25.949120\n",
      "Iteration 1217 => Loss: 25.931353\n",
      "Iteration 1218 => Loss: 25.913787\n",
      "Iteration 1219 => Loss: 25.896420\n",
      "Iteration 1220 => Loss: 25.879253\n",
      "Iteration 1221 => Loss: 25.862287\n",
      "Iteration 1222 => Loss: 25.845520\n",
      "Iteration 1223 => Loss: 25.828953\n",
      "Iteration 1224 => Loss: 25.827287\n",
      "Iteration 1225 => Loss: 25.808387\n",
      "Iteration 1226 => Loss: 25.789687\n",
      "Iteration 1227 => Loss: 25.771187\n",
      "Iteration 1228 => Loss: 25.752887\n",
      "Iteration 1229 => Loss: 25.734787\n",
      "Iteration 1230 => Loss: 25.716887\n",
      "Iteration 1231 => Loss: 25.699187\n",
      "Iteration 1232 => Loss: 25.681687\n",
      "Iteration 1233 => Loss: 25.664387\n",
      "Iteration 1234 => Loss: 25.647287\n",
      "Iteration 1235 => Loss: 25.630387\n",
      "Iteration 1236 => Loss: 25.613687\n",
      "Iteration 1237 => Loss: 25.597187\n",
      "Iteration 1238 => Loss: 25.580887\n",
      "Iteration 1239 => Loss: 25.564787\n",
      "Iteration 1240 => Loss: 25.548887\n",
      "Iteration 1241 => Loss: 25.533187\n",
      "Iteration 1242 => Loss: 25.532213\n",
      "Iteration 1243 => Loss: 25.514180\n",
      "Iteration 1244 => Loss: 25.496347\n",
      "Iteration 1245 => Loss: 25.478713\n",
      "Iteration 1246 => Loss: 25.461280\n",
      "Iteration 1247 => Loss: 25.444047\n",
      "Iteration 1248 => Loss: 25.427013\n",
      "Iteration 1249 => Loss: 25.410180\n",
      "Iteration 1250 => Loss: 25.393547\n",
      "Iteration 1251 => Loss: 25.377113\n",
      "Iteration 1252 => Loss: 25.360880\n",
      "Iteration 1253 => Loss: 25.344847\n",
      "Iteration 1254 => Loss: 25.329013\n",
      "Iteration 1255 => Loss: 25.313380\n",
      "Iteration 1256 => Loss: 25.297947\n",
      "Iteration 1257 => Loss: 25.282713\n",
      "Iteration 1258 => Loss: 25.267680\n",
      "Iteration 1259 => Loss: 25.252847\n",
      "Iteration 1260 => Loss: 25.252567\n",
      "Iteration 1261 => Loss: 25.235400\n",
      "Iteration 1262 => Loss: 25.218433\n",
      "Iteration 1263 => Loss: 25.201667\n",
      "Iteration 1264 => Loss: 25.185100\n",
      "Iteration 1265 => Loss: 25.168733\n",
      "Iteration 1266 => Loss: 25.152567\n",
      "Iteration 1267 => Loss: 25.136600\n",
      "Iteration 1268 => Loss: 25.120833\n",
      "Iteration 1269 => Loss: 25.105267\n",
      "Iteration 1270 => Loss: 25.089900\n",
      "Iteration 1271 => Loss: 25.074733\n",
      "Iteration 1272 => Loss: 25.059767\n",
      "Iteration 1273 => Loss: 25.045000\n",
      "Iteration 1274 => Loss: 25.030433\n",
      "Iteration 1275 => Loss: 25.016067\n",
      "Iteration 1276 => Loss: 25.001900\n",
      "Iteration 1277 => Loss: 24.987933\n",
      "Iteration 1278 => Loss: 24.974167\n",
      "Iteration 1279 => Loss: 24.972047\n",
      "Iteration 1280 => Loss: 24.955947\n",
      "Iteration 1281 => Loss: 24.940047\n",
      "Iteration 1282 => Loss: 24.924347\n",
      "Iteration 1283 => Loss: 24.908847\n",
      "Iteration 1284 => Loss: 24.893547\n",
      "Iteration 1285 => Loss: 24.878447\n",
      "Iteration 1286 => Loss: 24.863547\n",
      "Iteration 1287 => Loss: 24.848847\n",
      "Iteration 1288 => Loss: 24.834347\n",
      "Iteration 1289 => Loss: 24.820047\n",
      "Iteration 1290 => Loss: 24.805947\n",
      "Iteration 1291 => Loss: 24.792047\n",
      "Iteration 1292 => Loss: 24.778347\n",
      "Iteration 1293 => Loss: 24.764847\n",
      "Iteration 1294 => Loss: 24.751547\n",
      "Iteration 1295 => Loss: 24.738447\n",
      "Iteration 1296 => Loss: 24.725547\n",
      "Iteration 1297 => Loss: 24.724120\n",
      "Iteration 1298 => Loss: 24.708887\n",
      "Iteration 1299 => Loss: 24.693853\n",
      "Iteration 1300 => Loss: 24.679020\n",
      "Iteration 1301 => Loss: 24.664387\n",
      "Iteration 1302 => Loss: 24.649953\n",
      "Iteration 1303 => Loss: 24.635720\n",
      "Iteration 1304 => Loss: 24.621687\n",
      "Iteration 1305 => Loss: 24.607853\n",
      "Iteration 1306 => Loss: 24.594220\n",
      "Iteration 1307 => Loss: 24.580787\n",
      "Iteration 1308 => Loss: 24.567553\n",
      "Iteration 1309 => Loss: 24.554520\n",
      "Iteration 1310 => Loss: 24.541687\n",
      "Iteration 1311 => Loss: 24.529053\n",
      "Iteration 1312 => Loss: 24.516620\n",
      "Iteration 1313 => Loss: 24.504387\n",
      "Iteration 1314 => Loss: 24.492353\n",
      "Iteration 1315 => Loss: 24.491620\n",
      "Iteration 1316 => Loss: 24.477253\n",
      "Iteration 1317 => Loss: 24.463087\n",
      "Iteration 1318 => Loss: 24.449120\n",
      "Iteration 1319 => Loss: 24.435353\n",
      "Iteration 1320 => Loss: 24.421787\n",
      "Iteration 1321 => Loss: 24.408420\n",
      "Iteration 1322 => Loss: 24.395253\n",
      "Iteration 1323 => Loss: 24.382287\n",
      "Iteration 1324 => Loss: 24.369520\n",
      "Iteration 1325 => Loss: 24.356953\n",
      "Iteration 1326 => Loss: 24.344587\n",
      "Iteration 1327 => Loss: 24.332420\n",
      "Iteration 1328 => Loss: 24.320453\n",
      "Iteration 1329 => Loss: 24.308687\n",
      "Iteration 1330 => Loss: 24.297120\n",
      "Iteration 1331 => Loss: 24.285753\n",
      "Iteration 1332 => Loss: 24.274587\n",
      "Iteration 1333 => Loss: 24.274547\n",
      "Iteration 1334 => Loss: 24.261047\n",
      "Iteration 1335 => Loss: 24.247747\n",
      "Iteration 1336 => Loss: 24.234647\n",
      "Iteration 1337 => Loss: 24.221747\n",
      "Iteration 1338 => Loss: 24.209047\n",
      "Iteration 1339 => Loss: 24.196547\n",
      "Iteration 1340 => Loss: 24.184247\n",
      "Iteration 1341 => Loss: 24.172147\n",
      "Iteration 1342 => Loss: 24.160247\n",
      "Iteration 1343 => Loss: 24.148547\n",
      "Iteration 1344 => Loss: 24.137047\n",
      "Iteration 1345 => Loss: 24.125747\n",
      "Iteration 1346 => Loss: 24.114647\n",
      "Iteration 1347 => Loss: 24.103747\n",
      "Iteration 1348 => Loss: 24.093047\n",
      "Iteration 1349 => Loss: 24.082547\n",
      "Iteration 1350 => Loss: 24.072247\n",
      "Iteration 1351 => Loss: 24.062147\n",
      "Iteration 1352 => Loss: 24.060267\n",
      "Iteration 1353 => Loss: 24.047833\n",
      "Iteration 1354 => Loss: 24.035600\n",
      "Iteration 1355 => Loss: 24.023567\n",
      "Iteration 1356 => Loss: 24.011733\n",
      "Iteration 1357 => Loss: 24.000100\n",
      "Iteration 1358 => Loss: 23.988667\n",
      "Iteration 1359 => Loss: 23.977433\n",
      "Iteration 1360 => Loss: 23.966400\n",
      "Iteration 1361 => Loss: 23.955567\n",
      "Iteration 1362 => Loss: 23.944933\n",
      "Iteration 1363 => Loss: 23.934500\n",
      "Iteration 1364 => Loss: 23.924267\n",
      "Iteration 1365 => Loss: 23.914233\n",
      "Iteration 1366 => Loss: 23.904400\n",
      "Iteration 1367 => Loss: 23.894767\n",
      "Iteration 1368 => Loss: 23.885333\n",
      "Iteration 1369 => Loss: 23.876100\n",
      "Iteration 1370 => Loss: 23.874913\n",
      "Iteration 1371 => Loss: 23.863347\n",
      "Iteration 1372 => Loss: 23.851980\n",
      "Iteration 1373 => Loss: 23.840813\n",
      "Iteration 1374 => Loss: 23.829847\n",
      "Iteration 1375 => Loss: 23.819080\n",
      "Iteration 1376 => Loss: 23.808513\n",
      "Iteration 1377 => Loss: 23.798147\n",
      "Iteration 1378 => Loss: 23.787980\n",
      "Iteration 1379 => Loss: 23.778013\n",
      "Iteration 1380 => Loss: 23.768247\n",
      "Iteration 1381 => Loss: 23.758680\n",
      "Iteration 1382 => Loss: 23.749313\n",
      "Iteration 1383 => Loss: 23.740147\n",
      "Iteration 1384 => Loss: 23.731180\n",
      "Iteration 1385 => Loss: 23.722413\n",
      "Iteration 1386 => Loss: 23.713847\n",
      "Iteration 1387 => Loss: 23.705480\n",
      "Iteration 1388 => Loss: 23.704987\n",
      "Iteration 1389 => Loss: 23.694287\n",
      "Iteration 1390 => Loss: 23.683787\n",
      "Iteration 1391 => Loss: 23.673487\n",
      "Iteration 1392 => Loss: 23.663387\n",
      "Iteration 1393 => Loss: 23.653487\n",
      "Iteration 1394 => Loss: 23.643787\n",
      "Iteration 1395 => Loss: 23.634287\n",
      "Iteration 1396 => Loss: 23.624987\n",
      "Iteration 1397 => Loss: 23.615887\n",
      "Iteration 1398 => Loss: 23.606987\n",
      "Iteration 1399 => Loss: 23.598287\n",
      "Iteration 1400 => Loss: 23.589787\n",
      "Iteration 1401 => Loss: 23.581487\n",
      "Iteration 1402 => Loss: 23.573387\n",
      "Iteration 1403 => Loss: 23.565487\n",
      "Iteration 1404 => Loss: 23.557787\n",
      "Iteration 1405 => Loss: 23.550287\n",
      "Iteration 1406 => Loss: 23.542987\n",
      "Iteration 1407 => Loss: 23.540653\n",
      "Iteration 1408 => Loss: 23.531020\n",
      "Iteration 1409 => Loss: 23.521587\n",
      "Iteration 1410 => Loss: 23.512353\n",
      "Iteration 1411 => Loss: 23.503320\n",
      "Iteration 1412 => Loss: 23.494487\n",
      "Iteration 1413 => Loss: 23.485853\n",
      "Iteration 1414 => Loss: 23.477420\n",
      "Iteration 1415 => Loss: 23.469187\n",
      "Iteration 1416 => Loss: 23.461153\n",
      "Iteration 1417 => Loss: 23.453320\n",
      "Iteration 1418 => Loss: 23.445687\n",
      "Iteration 1419 => Loss: 23.438253\n",
      "Iteration 1420 => Loss: 23.431020\n",
      "Iteration 1421 => Loss: 23.423987\n",
      "Iteration 1422 => Loss: 23.417153\n",
      "Iteration 1423 => Loss: 23.410520\n",
      "Iteration 1424 => Loss: 23.404087\n",
      "Iteration 1425 => Loss: 23.402447\n",
      "Iteration 1426 => Loss: 23.393680\n",
      "Iteration 1427 => Loss: 23.385113\n",
      "Iteration 1428 => Loss: 23.376747\n",
      "Iteration 1429 => Loss: 23.368580\n",
      "Iteration 1430 => Loss: 23.360613\n",
      "Iteration 1431 => Loss: 23.352847\n",
      "Iteration 1432 => Loss: 23.345280\n",
      "Iteration 1433 => Loss: 23.337913\n",
      "Iteration 1434 => Loss: 23.330747\n",
      "Iteration 1435 => Loss: 23.323780\n",
      "Iteration 1436 => Loss: 23.317013\n",
      "Iteration 1437 => Loss: 23.310447\n",
      "Iteration 1438 => Loss: 23.304080\n",
      "Iteration 1439 => Loss: 23.297913\n",
      "Iteration 1440 => Loss: 23.291947\n",
      "Iteration 1441 => Loss: 23.286180\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1442 => Loss: 23.280613\n",
      "Iteration 1443 => Loss: 23.279667\n",
      "Iteration 1444 => Loss: 23.271767\n",
      "Iteration 1445 => Loss: 23.264067\n",
      "Iteration 1446 => Loss: 23.256567\n",
      "Iteration 1447 => Loss: 23.249267\n",
      "Iteration 1448 => Loss: 23.242167\n",
      "Iteration 1449 => Loss: 23.235267\n",
      "Iteration 1450 => Loss: 23.228567\n",
      "Iteration 1451 => Loss: 23.222067\n",
      "Iteration 1452 => Loss: 23.215767\n",
      "Iteration 1453 => Loss: 23.209667\n",
      "Iteration 1454 => Loss: 23.203767\n",
      "Iteration 1455 => Loss: 23.198067\n",
      "Iteration 1456 => Loss: 23.192567\n",
      "Iteration 1457 => Loss: 23.187267\n",
      "Iteration 1458 => Loss: 23.182167\n",
      "Iteration 1459 => Loss: 23.177267\n",
      "Iteration 1460 => Loss: 23.172567\n",
      "Iteration 1461 => Loss: 23.172313\n",
      "Iteration 1462 => Loss: 23.165280\n",
      "Iteration 1463 => Loss: 23.158447\n",
      "Iteration 1464 => Loss: 23.151813\n",
      "Iteration 1465 => Loss: 23.145380\n",
      "Iteration 1466 => Loss: 23.139147\n",
      "Iteration 1467 => Loss: 23.133113\n",
      "Iteration 1468 => Loss: 23.127280\n",
      "Iteration 1469 => Loss: 23.121647\n",
      "Iteration 1470 => Loss: 23.116213\n",
      "Iteration 1471 => Loss: 23.110980\n",
      "Iteration 1472 => Loss: 23.105947\n",
      "Iteration 1473 => Loss: 23.101113\n",
      "Iteration 1474 => Loss: 23.096480\n",
      "Iteration 1475 => Loss: 23.092047\n",
      "Iteration 1476 => Loss: 23.087813\n",
      "Iteration 1477 => Loss: 23.083780\n",
      "Iteration 1478 => Loss: 23.079947\n",
      "Iteration 1479 => Loss: 23.076313\n",
      "Iteration 1480 => Loss: 23.074220\n",
      "Iteration 1481 => Loss: 23.068253\n",
      "Iteration 1482 => Loss: 23.062487\n",
      "Iteration 1483 => Loss: 23.056920\n",
      "Iteration 1484 => Loss: 23.051553\n",
      "Iteration 1485 => Loss: 23.046387\n",
      "Iteration 1486 => Loss: 23.041420\n",
      "Iteration 1487 => Loss: 23.036653\n",
      "Iteration 1488 => Loss: 23.032087\n",
      "Iteration 1489 => Loss: 23.027720\n",
      "Iteration 1490 => Loss: 23.023553\n",
      "Iteration 1491 => Loss: 23.019587\n",
      "Iteration 1492 => Loss: 23.015820\n",
      "Iteration 1493 => Loss: 23.012253\n",
      "Iteration 1494 => Loss: 23.008887\n",
      "Iteration 1495 => Loss: 23.005720\n",
      "Iteration 1496 => Loss: 23.002753\n",
      "Iteration 1497 => Loss: 22.999987\n",
      "Iteration 1498 => Loss: 22.998587\n",
      "Iteration 1499 => Loss: 22.993487\n",
      "Iteration 1500 => Loss: 22.988587\n",
      "Iteration 1501 => Loss: 22.983887\n",
      "Iteration 1502 => Loss: 22.979387\n",
      "Iteration 1503 => Loss: 22.975087\n",
      "Iteration 1504 => Loss: 22.970987\n",
      "Iteration 1505 => Loss: 22.967087\n",
      "Iteration 1506 => Loss: 22.963387\n",
      "Iteration 1507 => Loss: 22.959887\n",
      "Iteration 1508 => Loss: 22.956587\n",
      "Iteration 1509 => Loss: 22.953487\n",
      "Iteration 1510 => Loss: 22.950587\n",
      "Iteration 1511 => Loss: 22.947887\n",
      "Iteration 1512 => Loss: 22.945387\n",
      "Iteration 1513 => Loss: 22.943087\n",
      "Iteration 1514 => Loss: 22.940987\n",
      "Iteration 1515 => Loss: 22.939087\n",
      "Iteration 1516 => Loss: 22.938380\n",
      "Iteration 1517 => Loss: 22.934147\n",
      "Iteration 1518 => Loss: 22.930113\n",
      "Iteration 1519 => Loss: 22.926280\n",
      "Iteration 1520 => Loss: 22.922647\n",
      "Iteration 1521 => Loss: 22.919213\n",
      "Iteration 1522 => Loss: 22.915980\n",
      "Iteration 1523 => Loss: 22.912947\n",
      "Iteration 1524 => Loss: 22.910113\n",
      "Iteration 1525 => Loss: 22.907480\n",
      "Iteration 1526 => Loss: 22.905047\n",
      "Iteration 1527 => Loss: 22.902813\n",
      "Iteration 1528 => Loss: 22.900780\n",
      "Iteration 1529 => Loss: 22.898947\n",
      "Iteration 1530 => Loss: 22.897313\n",
      "Iteration 1531 => Loss: 22.895880\n",
      "Iteration 1532 => Loss: 22.894647\n",
      "Iteration 1533 => Loss: 22.893613\n",
      "Iteration 1534 => Loss: 22.893600\n",
      "Iteration 1535 => Loss: 22.890233\n",
      "Iteration 1536 => Loss: 22.887067\n",
      "Iteration 1537 => Loss: 22.884100\n",
      "Iteration 1538 => Loss: 22.881333\n",
      "Iteration 1539 => Loss: 22.878767\n",
      "Iteration 1540 => Loss: 22.876400\n",
      "Iteration 1541 => Loss: 22.874233\n",
      "Iteration 1542 => Loss: 22.872267\n",
      "Iteration 1543 => Loss: 22.870500\n",
      "Iteration 1544 => Loss: 22.868933\n",
      "Iteration 1545 => Loss: 22.867567\n",
      "Iteration 1546 => Loss: 22.866400\n",
      "Iteration 1547 => Loss: 22.865433\n",
      "Iteration 1548 => Loss: 22.864667\n",
      "Iteration 1549 => Loss: 22.864100\n",
      "Iteration 1550 => Loss: 22.863733\n",
      "Iteration 1551 => Loss: 22.863567\n"
     ]
    }
   ],
   "source": [
    "w, b = train(X, Y, 10000, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.xlabel(\"Reservations\",fontsize=20)\n",
    "plt.ylabel(\"Pizzas\",fontsize=20)\n",
    "plt.axis([0,50,0,50])\n",
    "plt.plot(X,Y,\"bo\")\n",
    "plt.plot([0, 50], [b, predict(50, w, b)], color=\"g\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59.1299999999998"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reservations = 42\n",
    "predict(reservations, w, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
